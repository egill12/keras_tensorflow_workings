{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (2.2.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from keras) (1.0.8)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.7/site-packages (from keras) (1.3.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from keras) (5.1.2)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras) (1.17.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from keras) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (1.14.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.22.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.1.7)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.33.4)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.8.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.7.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow) (2.9.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (41.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.15.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "standardise_data() missing 1 required positional argument: 'window'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fd4ce6037a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# standardise the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdata_normed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardise_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mdata_normed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdata_normed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CCY'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CCY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: standardise_data() missing 1 required positional argument: 'window'"
     ]
    }
   ],
   "source": [
    "# This file is a simple implementation of the \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_trading_module import create_train_test_file, create_dataset, get_accuracy, signal\n",
    "import datetime\n",
    "\n",
    "model = Sequential()\n",
    "# data_file = pd.read_csv(r\"/storage/eurusd_train_normed.csv\")\n",
    "data_file = pd.read_csv(r\"/storage/ccyData.csv\")\n",
    "#eurusd_test = pd.read_csv(r\"/storage/eurusd_test_normed.csv\")\n",
    "data_file = data_file.replace(np.nan, 0)\n",
    "#eurusd_test = eurusd_test.replace(np.nan, 0)\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" :[], \"epochs\": [] , \"Info_Ratio\" : [], \"run_time\" : []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "look_back = 66 # [21, 66]\n",
    "trade_horizon = 24\n",
    "model_features = [\"spot_v_HF\", \"spot_v_MF\", \"spot_v_LF\", \"HF_ema_diff\",\n",
    "                   \"MF_ema_diff\",\"LF_ema_diff\", \"LDN\", \"NY\", \"Asia\", \"target\"]\n",
    "data_size = 2500\n",
    "test_split = 0.75\n",
    "# roughly 3 yrs of data slightly less actually\n",
    "window = 15000\n",
    "# the length of our prediction\n",
    "\n",
    "# standardise the data\n",
    "data_normed = standardise_data(data_file, model_features, window)\n",
    "data_normed['Date'] = data_file['Date']\n",
    "data_normed['CCY'] = data_file['CCY']\n",
    "data_normed['logret'] = data_file['logret']\n",
    "data_normed['LDN'] = data_file['LDN']\n",
    "data_normed['NY'] = data_file['NY']\n",
    "data_normed['Asia'] = data_file['Asia']\n",
    "data_normed['target_raw'] = data_file['target']\n",
    "data_normed['target'] = data_file['target_binary']\n",
    "\n",
    "# create data_set\n",
    "train , test = create_train_test_file(data_file, data_size, test_split)\n",
    "train_sample = train[model_features].values\n",
    "test_sample = test[model_features].values\n",
    " # Parse the values into the LSTM format\n",
    "train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "\n",
    "# reshape seems to add another list around every observation\n",
    "train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "\n",
    "# Build up the model\n",
    "BATCH_SIZE = 300\n",
    "no_features = train_data.shape[2]\n",
    "model = Sequential()\n",
    "model.add(LSTM(4,batch_input_shape = (None,look_back,no_features), return_sequences = True))\n",
    "model.add(LSTM(1, return_sequences = False, activation=\"tanh\"))\n",
    "model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "\n",
    "EPOCH = 1000\n",
    "# train the model\n",
    "# verbose = 1 gives the output of the training.\n",
    "start_time = datetime.datetime.now()\n",
    "lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose=1)\n",
    "run_time = datetime.datetime.now() - start_time\n",
    "# run training on the test data\n",
    "results = model.predict(test_data)\n",
    "# The % threshold needed to trigger a signal either way\n",
    "thold = 0.55\n",
    "predicted = [signal(i, thold) for i in results]\n",
    "acc_score = get_accuracy(predicted, test_target)\n",
    "predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "# calculate the returns of the signal\n",
    "test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "# no shift needed as we have already done that in previous step\n",
    "test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "strat_return = test_results['strat_returns'].sum()\n",
    "information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "\n",
    "# Store the data as needed\n",
    "performance_store['data_size'].append(data_size)\n",
    "performance_store['epochs'].append(EPOCH)\n",
    "performance_store['Accuracy_Score'].append(acc_score)\n",
    "performance_store['Info_Ratio'].append(information_ratio)\n",
    "performance_store['run_time'].append(run_time)\n",
    "performance_df = pd.DataFrame(performance_store)\n",
    "save_results = r\"/storage/test_result_size%s_lkbk%s_Epochs%s_thold%s.csv\" % (data_size, look_back, EPOCH, thold)\n",
    "test_results.to_csv( save_results,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "initialise_process() missing 1 required positional argument: 'use_random_train_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-45463dde5c83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0muse_separated_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m# Use a rolling window to train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m data_normed, model_features, features_to_standardise = initialise_process(file_location, trade_horizon, \n\u001b[0;32m---> 38\u001b[0;31m                                                  window, use_risk_adjusted, use_pca)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;31m################ Loop through the full dataset in terms of the training and testing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mstart_row\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdata_normed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: initialise_process() missing 1 required positional argument: 'use_random_train_data'"
     ]
    }
   ],
   "source": [
    "# This file is a simple implementation of the \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_trading_module import create_train_test_file, create_dataset, get_accuracy, signal\n",
    "import datetime\n",
    "\n",
    "file_location = r\"/storage/ccyData.csv\"\n",
    "\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"epochs\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "look_back = 60 # [21, 66]\n",
    "trade_horizon = 24\n",
    "test_buffer = 5\n",
    "data_size = 5000\n",
    "test_split = 0.5\n",
    "total_data_needed = int(data_size*(1 + test_split)) + test_buffer\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = False # if True: training on the sharpe return else raw\n",
    "concat_results = True \n",
    "use_classifier = True\n",
    "################### Standardise Entire Dataset using rolling lookback windows #######\n",
    "# roughly ~3 yrs of data \n",
    "window = 17500\n",
    "use_pca = 3 # if = 0 then implies do not use pca in the model\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "start_row = 0\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "start_row = 85000\n",
    "use_separated_chunk = False # Use a rolling window to train and test\n",
    "data_normed, model_features, features_to_standardise = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca)\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "while start_row < data_normed.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += total_data_needed\n",
    "    if concat_results:\n",
    "        # in this instance, we can to add to the start row first before chunking the data\n",
    "        start_row += test_split\n",
    "        # we are training on all data available up until that point, and testing x timeperiods ahead\n",
    "        trunc_data = data_normed.loc[:start_row,:]  \n",
    "    else:\n",
    "        # this rolls the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += data_size\n",
    "    # standardise the data\n",
    "    #################### Set up training and testing ########################\n",
    "    \n",
    "    # create data_set\n",
    "    train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer)\n",
    "    if test.shape[0] <= (look_back+test_buffer+trade_horizon):\n",
    "        break\n",
    "    if use_pca > 0:\n",
    "        train, test, var_explained = get_pca_features(train,test, features_to_standardise, use_pca)\n",
    "    train_sample = train[model_features].values\n",
    "    test_sample = test[model_features].values\n",
    "\n",
    "     # Parse the values into the LSTM format\n",
    "    train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "    test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "    \n",
    "    # reshape seems to add another list around every observation\n",
    "    train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "    train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "    test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "    test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "    #### Set up model parameters\n",
    "    # Build up the model\n",
    "    BATCH_SIZE = 300\n",
    "    no_features = train_data.shape[2]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4,batch_input_shape = (None,look_back,no_features), return_sequences = True))\n",
    "    model.add(LSTM(1, return_sequences = False, activation=\"softmax\"))\n",
    "    model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    \n",
    "    EPOCH = 350\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    start_time = datetime.datetime.now()\n",
    "    lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose= 1)\n",
    "    run_time = datetime.datetime.now() - start_time\n",
    "    # run training on the test data\n",
    "    results = model.predict(test_data)\n",
    "    # The % threshold needed to trigger a signal either way\n",
    "    thold = 0.55\n",
    "    predicted = [signal(i, thold) for i in results]\n",
    "    acc_score = get_accuracy(predicted, test_target)\n",
    "    # This needs to change to handle the change in the target\n",
    "    predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "    test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "    # calculate the returns of the signal\n",
    "    test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "    # no shift needed as we have already done that in previous step\n",
    "    test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "    test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "    strat_return = test_results['strat_returns'].sum()\n",
    "    information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "    \n",
    "    # Store the data as needed\n",
    "    performance_store['data_size'].append(data_size)\n",
    "    performance_store['epochs'].append(EPOCH)\n",
    "    performance_store['Accuracy_Score'].append(acc_score)\n",
    "    performance_store['Info_Ratio'].append(information_ratio)\n",
    "    performance_store['run_time'].append(run_time)\n",
    "    performance_store['train_date_st'].append(trunc_data['Date'].iloc[0])\n",
    "    performance_store['test_date_st'].append(test_results['Date'].iloc[0])\n",
    "    performance_df = pd.DataFrame(performance_store)\n",
    "    save_results = r\"/storage/test_result_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)\n",
    "    test_results.to_csv(save_results,index = False)\n",
    "    performance_df.to_csv(r\"/storage/performance_df_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paperspace code for looping through the data series\n",
    "\n",
    "model = Sequential()\n",
    "###################### Import Data ##################################\n",
    "# data_file = pd.read_csv(r\"/storage/eurusd_train_normed.csv\")\n",
    "data_file = pd.read_csv(r\"/storage/ccyData.csv\")\n",
    "data_file = data_file.replace(np.nan, 0)\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"epochs\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "###################### Add Parameters ##################################################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "look_back = 66 # [21, 66]\n",
    "trade_horizon = 24\n",
    "model_features = [\"spot_v_HF\", \"spot_v_MF\", \"spot_v_LF\", \"HF_ema_diff\",\n",
    "                   \"MF_ema_diff\",\"LF_ema_diff\", \"LDN\", \"NY\", \"Asia\", \"target\"]\n",
    "test_buffer = 5\n",
    "data_size = 2500\n",
    "test_split = 0.75\n",
    "total_data_needed = int(data_size*(1 + test_split)) + test_buffer\n",
    "\n",
    "while start_row < data_file.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_file.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_file.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        start_row += total_data_needed\n",
    "        trunc_data = data_file.loc[start_row:,:]\n",
    "    if use_rolling_chunk:\n",
    "        # this rools the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        start_row += data_size\n",
    "        trunc_data = data_file.loc[start_row:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random starting, EPOCH 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0915 14:40:29.788167 140096357189440 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0915 14:40:29.832605 140096357189440 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0915 14:40:29.848758 140096357189440 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0915 14:40:30.513771 140096357189440 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0915 14:40:30.925530 140096357189440 deprecation.py:323] From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0915 14:40:32.437649 140096357189440 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "W0915 14:40:32.620513 140096357189440 deprecation_wrapper.py:119] From /opt/conda/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3911 samples, validate on 111552 samples\n",
      "Epoch 1/1\n",
      "3911/3911 [==============================] - 82s 21ms/step - loss: 0.0018 - acc: 0.0064 - val_loss: 1.2364e-04 - val_acc: 5.3787e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111552/111552 [==============================] - 67s 601us/step\n"
     ]
    }
   ],
   "source": [
    "##### FOR PCA features  && train on random data!####\n",
    "# This file is a simple implementation of the \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_trading_module import *\n",
    "from model_functions import *\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "file_location = r\"/storage/ccyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"epochs\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "params_dict = set_params_random_forests()\n",
    "lstm_dict = set_params_LSTM()\n",
    "use_random_train_data = params_dict['use_random_train_data']\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "EPOCH = lstm_dict['EPOCH']\n",
    "first_layer = lstm_dict['first_layer']\n",
    "second_layer = lstm_dict['second_layer']\n",
    "look_back = lstm_dict['look_back'] \n",
    "\n",
    "# [i for i in range(25,301,25)] # [21, 66]\n",
    "# if running pca, max features can only be same or less than the full total of features\n",
    "test_buffer = params_dict['test_buffer']\n",
    "data_size = params_dict['data_size'] #  initially using 1500 training points\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = params_dict['concat_results']\n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = params_dict['test_split'] # roughly one month test ahead, which is a one month retrain period\n",
    "# signal threshold, when using classifier\n",
    "thold = params_dict['thold']\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = params_dict['window']\n",
    "###### Set Targets ##############\n",
    "trade_horizon = params_dict['trade_horizon'] # in hours\n",
    "use_risk_adjusted = params_dict['use_risk_adjusted'] # if True: training on the sharpe return else raw\n",
    "use_binary = params_dict['use_binary'] # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = params_dict['use_classifier']\n",
    "use_pca = params_dict['use_pca'] # if = 0 then implies do not use pca in the model\n",
    "use_separated_chunk = params_dict['use_separated_chunk']\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features, features_to_standardise = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca,use_random_train_data, use_binary)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = data_size\n",
    " # Use a rolling window to train and test\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "if use_random_train_data:\n",
    "    print(\"random starting, EPOCH %s\" % str(EPOCH) )\n",
    "    random_data_location = r\"/storage/CcyRandomTrendLSTM.csv\"\n",
    "    train, model_features, features_to_standardise = initialise_process(random_data_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, use_binary,use_random_train_data)\n",
    "    test, model_features, features_to_standardise  = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, \n",
    "                                                 use_binary ,use_random_train_data = False)\n",
    "    if test.shape[0] <= (look_back+test_buffer+trade_horizon):\n",
    "        sys.exit()\n",
    "    if use_pca > 0:\n",
    "        train, test, var_explained = get_pca_features(train,test, features_to_standardise, use_pca)\n",
    "    train_sample = train[model_features].values\n",
    "    test_sample = test[model_features].head(test.shape[0]-39).values # had to do this to make it work\n",
    "\n",
    "     # Parse the values into the LSTM format\n",
    "    train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "    test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "    \n",
    "    # reshape seems to add another list around every observation\n",
    "    train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "    train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "    test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "    test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "    #### Set up model parameters\n",
    "    # Build up the model\n",
    "    BATCH_SIZE = None # previous was 32 batch and 33 lookback\n",
    "    no_features = train_data.shape[2]\n",
    "    model = Sequential()\n",
    "    #model.add(LSTM(first_layer,batch_input_shape = (BATCH_SIZE,look_back,no_features), return_sequences = True))\n",
    "    #model.add(LSTM(second_layer, return_sequences = False, activation=\"softmax\"))\n",
    "    #model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    # newer method of learning\n",
    "    model.add(LSTM(first_layer, batch_input_shape = (BATCH_SIZE,look_back,no_features), return_sequences = True))\n",
    "    model.add(LSTM(second_layer, return_sequences = False, activation=\"softmax\"))\n",
    "    model.add(Dense(8, activation = \"tanh\"))\n",
    "    model.add(Dense(1, activation = \"linear\"))\n",
    "    model.compile(loss = \"mse\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    start_time = datetime.datetime.now()\n",
    "    lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose= 1)\n",
    "    #lstm_engine = model.fit(train_data,train_target,epochs = EPOCH, validation_data=(test_data,test_target),batch_size = BATCH_SIZE, verbose = 1)\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig , ax = plt.subplots(1,1,figsize=(10,8))\n",
    "    plt.plot(lstm_engine.history['loss'])\n",
    "    plt.show()\n",
    "    run_time = datetime.datetime.now() - start_time\n",
    "    # run training on the test data\n",
    "    results = model.predict(test_data, batch_size = BATCH_SIZE, verbose = 1)\n",
    "    # The % threshold needed to trigger a signal either way\n",
    "    predicted = [i[0] for i in results] # [np.sign(i) for i in results] #[signal(i, thold) for i in results]\n",
    "    acc_score = get_accuracy([np.sign(i) for i in results], test_target)\n",
    "    # This needs to change to handle the change in the target\n",
    "    predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "    test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "    # calculate the returns of the signal\n",
    "    test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "    # no shift needed as we have already done that in previous step\n",
    "    test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "    test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "    strat_return = test_results['strat_returns'].sum()\n",
    "    information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "    \n",
    "    # Store the data as needed\n",
    "    performance_store['data_size'].append(data_size)\n",
    "    performance_store['epochs'].append(EPOCH)\n",
    "    performance_store['Accuracy_Score'].append(acc_score)\n",
    "    performance_store['Info_Ratio'].append(information_ratio)\n",
    "    performance_store['run_time'].append(run_time)\n",
    "    performance_store['train_date_st'].append(test_results['Date'].iloc[0])\n",
    "    performance_store['test_date_st'].append(test_results['Date'].iloc[-1])\n",
    "    performance_df = pd.DataFrame(performance_store)\n",
    "    save_results = r\"/storage/test_result_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)\n",
    "    test_results.to_csv(save_results,index = False)\n",
    "    performance_df.to_csv(r\"/storage/performance_df_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold))\n",
    "else:\n",
    "    # if not using random data then move to the normal method.\n",
    "    ################ Loop through the full dataset in terms of the training and testing.\n",
    "    while start_row < data_normed.shape[0]:\n",
    "        # first check if there is enough data left\n",
    "        if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "            # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "            trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "        # we need to increment over the data size\n",
    "        if use_separated_chunk:\n",
    "            # this means we jump across the full previous train and test data\n",
    "            trunc_data = data_normed.loc[start_row:,:]\n",
    "            start_row += total_data_needed\n",
    "        if concat_results:\n",
    "            # in this instance, we can to add to the start row first before chunking the data\n",
    "            start_row += test_split\n",
    "            # we are training on all data available up until that point, and testing x timeperiods ahead\n",
    "            trunc_data = data_normed.loc[:start_row,:]  \n",
    "        else:\n",
    "            # this rolls the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "            trunc_data = data_normed.loc[start_row:,:]\n",
    "            start_row += data_size\n",
    "        # standardise the data\n",
    "        #################### Set up training and testing ########################\n",
    "        \n",
    "        # create data_set\n",
    "        train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer)\n",
    "        if test.shape[0] <= (look_back+test_buffer+trade_horizon):\n",
    "            break\n",
    "        if use_pca > 0:\n",
    "            train, test, var_explained = get_pca_features(train,test, features_to_standardise, use_pca)\n",
    "        train_sample = train[model_features].values\n",
    "        test_sample = test[model_features].values\n",
    "    \n",
    "         # Parse the values into the LSTM format\n",
    "        train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "        test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "        \n",
    "        # reshape seems to add another list around every observation\n",
    "        train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "        train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "        test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "        test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "        #### Set up model parameters\n",
    "        # Build up the model\n",
    "        BATCH_SIZE = 6\n",
    "        no_features = train_data.shape[2]\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(120,batch_input_shape = (None,look_back,no_features), return_sequences = True))\n",
    "        model.add(LSTM(1, return_sequences = False, activation=\"softmax\"))\n",
    "        model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "        \n",
    "        EPOCH = 350\n",
    "        # train the model\n",
    "        # verbose = 1 gives the output of the training.\n",
    "        start_time = datetime.datetime.now()\n",
    "        lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose= 1)\n",
    "        run_time = datetime.datetime.now() - start_time\n",
    "        # run training on the test data\n",
    "        results = model.predict(test_data)\n",
    "        # The % threshold needed to trigger a signal either way\n",
    "        #thold = 0.55\n",
    "        predicted = [np.sign(i) for i in results] # [signal(i, thold) for i in results]\n",
    "        acc_score = get_accuracy(predicted, test_target)\n",
    "        # This needs to change to handle the change in the target\n",
    "        predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "        test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "        # calculate the returns of the signal\n",
    "        test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "        # no shift needed as we have already done that in previous step\n",
    "        test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "        test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "        strat_return = test_results['strat_returns'].sum()\n",
    "        information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "        \n",
    "        # Store the data as needed\n",
    "        performance_store['data_size'].append(data_size)\n",
    "        performance_store['epochs'].append(EPOCH)\n",
    "        performance_store['Accuracy_Score'].append(acc_score)\n",
    "        performance_store['Info_Ratio'].append(information_ratio)\n",
    "        performance_store['run_time'].append(run_time)\n",
    "        performance_store['train_date_st'].append(test_results['Date'].iloc[0])\n",
    "        performance_store['test_date_st'].append(test_results['Date'].iloc[-1])\n",
    "        performance_df = pd.DataFrame(performance_store)\n",
    "        save_results = r\"/storage/test_result_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)\n",
    "        test_results.to_csv(save_results,index = False)\n",
    "        performance_df.to_csv(r\"/storage/performance_df_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# quick testing of what is happening with this LSTM\n",
    "import matplotlib.pyplot as plt\n",
    "#fig , ax = plt.subplots(1,1,figsize=(12,8))\n",
    "# lets see the history of the error update.\n",
    "# want a smooth learning line\n",
    "#ax.plot(lstm_engine.history['loss'])\n",
    "import matplotlib.pyplot as plt\n",
    "fig , ax = plt.subplots(1,1,figsize=(10,8))\n",
    "plt.plot(lstm_engine.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 5, 10, 11159, 22318, 55795, 111590]\n",
      "[1, 2, 5, 10, 17, 23, 34, 46, 85, 115, 170, 230, 391, 782, 1955, 3910]\n",
      "{1, 2, 10, 5}\n",
      "111551\n",
      "43.44444444444444\n"
     ]
    }
   ],
   "source": [
    "def factors(n):\n",
    "    factors = []\n",
    "    for i in range (1,n+1):\n",
    "        if (n % i) == 0:\n",
    "            factors.append(i)\n",
    "    return factors\n",
    "x = 90\n",
    "print(factors(((test.shape[0]) - x)))\n",
    "#print((test.shape[0]) - x)\n",
    "print(factors(4000-x))\n",
    "print(set(factors(4000-x)).intersection(set(factors((test.shape[0]) - x))))\n",
    "print(int(test.shape[0]-x-39))\n",
    "print((4000-x)/90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1303, 3909]\n",
      "[1, 2, 5, 10, 23, 25, 46, 50, 97, 115, 194, 230, 485, 575, 970, 1150, 2231, 2425, 4462, 4850, 11155, 22310, 55775, 111550]\n"
     ]
    }
   ],
   "source": [
    "z = 3909\n",
    "k = 111550\n",
    "print(factors(z))\n",
    "print(factors(k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
