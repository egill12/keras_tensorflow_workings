{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This file is a simple implementation of the \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_trading_module import create_train_test_file, create_dataset, get_accuracy, signal\n",
    "import datetime\n",
    "\n",
    "model = Sequential()\n",
    "# data_file = pd.read_csv(r\"/storage/eurusd_train_normed.csv\")\n",
    "data_file = pd.read_csv(r\"/storage/ccyData.csv\")\n",
    "#eurusd_test = pd.read_csv(r\"/storage/eurusd_test_normed.csv\")\n",
    "data_file = data_file.replace(np.nan, 0)\n",
    "#eurusd_test = eurusd_test.replace(np.nan, 0)\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" :[], \"epochs\": [] , \"Info_Ratio\" : [], \"run_time\" : []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "look_back = 66 # [21, 66]\n",
    "trade_horizon = 24\n",
    "model_features = [\"spot_v_HF\", \"spot_v_MF\", \"spot_v_LF\", \"HF_ema_diff\",\n",
    "                   \"MF_ema_diff\",\"LF_ema_diff\", \"LDN\", \"NY\", \"Asia\", \"target\"]\n",
    "data_size = 2500\n",
    "test_split = 0.75\n",
    "# roughly 3 yrs of data slightly less actually\n",
    "window = 15000\n",
    "# the length of our prediction\n",
    "\n",
    "# standardise the data\n",
    "data_normed = standardise_data(data_file, model_features, window)\n",
    "data_normed['Date'] = data_file['Date']\n",
    "data_normed['CCY'] = data_file['CCY']\n",
    "data_normed['logret'] = data_file['logret']\n",
    "data_normed['LDN'] = data_file['LDN']\n",
    "data_normed['NY'] = data_file['NY']\n",
    "data_normed['Asia'] = data_file['Asia']\n",
    "data_normed['target_raw'] = data_file['target']\n",
    "data_normed['target'] = data_file['target_binary']\n",
    "\n",
    "# create data_set\n",
    "train , test = create_train_test_file(data_file, data_size, test_split)\n",
    "train_sample = train[model_features].values\n",
    "test_sample = test[model_features].values\n",
    " # Parse the values into the LSTM format\n",
    "train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "\n",
    "# reshape seems to add another list around every observation\n",
    "train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "\n",
    "# Build up the model\n",
    "BATCH_SIZE = 300\n",
    "no_features = train_data.shape[2]\n",
    "model = Sequential()\n",
    "model.add(LSTM(4,batch_input_shape = (None,look_back,no_features), return_sequences = True))\n",
    "model.add(LSTM(1, return_sequences = False, activation=\"softmax\"))\n",
    "model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "\n",
    "EPOCH = 1000\n",
    "# train the model\n",
    "# verbose = 1 gives the output of the training.\n",
    "start_time = datetime.datetime.now()\n",
    "lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose=1)\n",
    "run_time = datetime.datetime.now() - start_time\n",
    "# run training on the test data\n",
    "results = model.predict(test_data)\n",
    "# The % threshold needed to trigger a signal either way\n",
    "thold = 0.55\n",
    "predicted = [signal(i, thold) for i in results]\n",
    "acc_score = get_accuracy(predicted, test_target)\n",
    "predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "# calculate the returns of the signal\n",
    "test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "# no shift needed as we have already done that in previous step\n",
    "test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "strat_return = test_results['strat_returns'].sum()\n",
    "information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "\n",
    "# Store the data as needed\n",
    "performance_store['data_size'].append(data_size)\n",
    "performance_store['epochs'].append(EPOCH)\n",
    "performance_store['Accuracy_Score'].append(acc_score)\n",
    "performance_store['Info_Ratio'].append(information_ratio)\n",
    "performance_store['run_time'].append(run_time)\n",
    "performance_df = pd.DataFrame(performance_store)\n",
    "save_results = r\"/storage/test_result_size%s_lkbk%s_Epochs%s_thold%s.csv\" % (data_size, look_back, EPOCH, thold)\n",
    "test_results.to_csv( save_results,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is a simple implementation of the \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_trading_module import create_train_test_file, create_dataset, get_accuracy, signal\n",
    "import datetime\n",
    "\n",
    "file_location = r\"/storage/ccyData.csv\"\n",
    "\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"epochs\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "look_back = 60 # [21, 66]\n",
    "trade_horizon = 24\n",
    "test_buffer = 5\n",
    "data_size = 5000\n",
    "test_split = 0.5\n",
    "total_data_needed = int(data_size*(1 + test_split)) + test_buffer\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = False # if True: training on the sharpe return else raw\n",
    "concat_results = True \n",
    "use_classifier = True\n",
    "################### Standardise Entire Dataset using rolling lookback windows #######\n",
    "# roughly ~3 yrs of data \n",
    "window = 17500\n",
    "use_pca = 3 # if = 0 then implies do not use pca in the model\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "start_row = 0\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "start_row = 85000\n",
    "use_separated_chunk = False # Use a rolling window to train and test\n",
    "data_normed, model_features, features_to_standardise = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca)\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "while start_row < data_normed.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += total_data_needed\n",
    "    if concat_results:\n",
    "        # in this instance, we can to add to the start row first before chunking the data\n",
    "        start_row += test_split\n",
    "        # we are training on all data available up until that point, and testing x timeperiods ahead\n",
    "        trunc_data = data_normed.loc[:start_row,:]  \n",
    "    else:\n",
    "        # this rolls the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += data_size\n",
    "    # standardise the data\n",
    "    #################### Set up training and testing ########################\n",
    "    \n",
    "    # create data_set\n",
    "    train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer)\n",
    "    if use_pca > 0:\n",
    "        train, test, var_explained = get_pca_features(train,test, features_to_standardise, use_pca)\n",
    "    train_sample = train[model_features].values\n",
    "    test_sample = test[model_features].values\n",
    "     # Parse the values into the LSTM format\n",
    "    train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "    test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "    \n",
    "    # reshape seems to add another list around every observation\n",
    "    train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "    train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "    test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "    test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "    #### Set up model parameters\n",
    "    # Build up the model\n",
    "    BATCH_SIZE = 300\n",
    "    no_features = train_data.shape[2]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4,batch_input_shape = (None,look_back,no_features), return_sequences = True))\n",
    "    model.add(LSTM(1, return_sequences = False, activation=\"softmax\"))\n",
    "    model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    \n",
    "    EPOCH = 350\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    start_time = datetime.datetime.now()\n",
    "    lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose= 1)\n",
    "    run_time = datetime.datetime.now() - start_time\n",
    "    # run training on the test data\n",
    "    results = model.predict(test_data)\n",
    "    # The % threshold needed to trigger a signal either way\n",
    "    thold = 0.55\n",
    "    predicted = [signal(i, thold) for i in results]\n",
    "    acc_score = get_accuracy(predicted, test_target)\n",
    "    # This needs to change to handle the change in the target\n",
    "    predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "    test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "    # calculate the returns of the signal\n",
    "    test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "    # no shift needed as we have already done that in previous step\n",
    "    test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "    test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "    strat_return = test_results['strat_returns'].sum()\n",
    "    information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "    \n",
    "    # Store the data as needed\n",
    "    performance_store['data_size'].append(data_size)\n",
    "    performance_store['epochs'].append(EPOCH)\n",
    "    performance_store['Accuracy_Score'].append(acc_score)\n",
    "    performance_store['Info_Ratio'].append(information_ratio)\n",
    "    performance_store['run_time'].append(run_time)\n",
    "    performance_store['train_date_st'].append(trunc_data['Date'].iloc[0])\n",
    "    performance_store['test_date_st'].append(test_results['Date'].iloc[0])\n",
    "    performance_df = pd.DataFrame(performance_store)\n",
    "    save_results = r\"/storage/test_result_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)\n",
    "    test_results.to_csv(save_results,index = False)\n",
    "    performance_df.to_csv(r\"/storage/performance_df_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paperspace code for looping through the data series\n",
    "\n",
    "model = Sequential()\n",
    "###################### Import Data ##################################\n",
    "# data_file = pd.read_csv(r\"/storage/eurusd_train_normed.csv\")\n",
    "data_file = pd.read_csv(r\"/storage/ccyData.csv\")\n",
    "data_file = data_file.replace(np.nan, 0)\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"epochs\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "###################### Add Parameters ##################################################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "look_back = 66 # [21, 66]\n",
    "trade_horizon = 24\n",
    "model_features = [\"spot_v_HF\", \"spot_v_MF\", \"spot_v_LF\", \"HF_ema_diff\",\n",
    "                   \"MF_ema_diff\",\"LF_ema_diff\", \"LDN\", \"NY\", \"Asia\", \"target\"]\n",
    "test_buffer = 5\n",
    "data_size = 2500\n",
    "test_split = 0.75\n",
    "total_data_needed = int(data_size*(1 + test_split)) + test_buffer\n",
    "\n",
    "while start_row < data_file.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_file.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_file.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        start_row += total_data_needed\n",
    "        trunc_data = data_file.loc[start_row:,:]\n",
    "    if use_rolling_chunk:\n",
    "        # this rools the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        start_row += data_size\n",
    "        trunc_data = data_file.loc[start_row:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mergeing signals to the df\n",
    "predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "test_original = pd.merge(test_original,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "test_original.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_horizon = 24\n",
    "test_original[\"scaled_signal\"] = test_original['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "# no shift needed as we have already done that inpervious step\n",
    "test_original['strat_returns'] = test_original['logret']*test_original['scaled_signal']\n",
    "test_original['strat_returns_sum'] = test_original['strat_returns'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = plt.plot()\n",
    "s1 = test_original['CCY']\n",
    "plt.plot(s1,'b')\n",
    "plt.ylabel('Ret',color='b')\n",
    "ax2 = plt.gca().twinx()\n",
    "s2 = test_original['strat_returns_sum'].loc[test_original['scaled_signal'] <0]\n",
    "ax2.plot(s2, 'r*')\n",
    "plt.ylabel('sin', color='r')\n",
    "s3 = test_original['strat_returns_sum'].loc[test_original['scaled_signal'] >0]\n",
    "ax2.plot(s3, 'g*')\n",
    "plt.show()\n",
    "test_original.to_csv(r\"/storage/signal_prediction_test.csv\",index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
