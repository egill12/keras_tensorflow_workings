{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This file is a simple implementation of the \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_trading_module import create_train_test_file, create_dataset, get_accuracy, signal\n",
    "import datetime\n",
    "\n",
    "model = Sequential()\n",
    "# data_file = pd.read_csv(r\"/storage/eurusd_train_normed.csv\")\n",
    "data_file = pd.read_csv(r\"/storage/ccyData.csv\")\n",
    "#eurusd_test = pd.read_csv(r\"/storage/eurusd_test_normed.csv\")\n",
    "data_file = data_file.replace(np.nan, 0)\n",
    "#eurusd_test = eurusd_test.replace(np.nan, 0)\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" :[], \"epochs\": [] , \"Info_Ratio\" : [], \"run_time\" : []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "look_back = 66 # [21, 66]\n",
    "trade_horizon = 24\n",
    "model_features = [\"spot_v_HF\", \"spot_v_MF\", \"spot_v_LF\", \"HF_ema_diff\",\n",
    "                   \"MF_ema_diff\",\"LF_ema_diff\", \"LDN\", \"NY\", \"Asia\", \"target\"]\n",
    "data_size = 2500\n",
    "test_split = 0.75\n",
    "# roughly 3 yrs of data slightly less actually\n",
    "window = 15000\n",
    "# the length of our prediction\n",
    "\n",
    "# standardise the data\n",
    "data_normed = standardise_data(data_file, model_features, window)\n",
    "data_normed['Date'] = data_file['Date']\n",
    "data_normed['CCY'] = data_file['CCY']\n",
    "data_normed['logret'] = data_file['logret']\n",
    "data_normed['LDN'] = data_file['LDN']\n",
    "data_normed['NY'] = data_file['NY']\n",
    "data_normed['Asia'] = data_file['Asia']\n",
    "data_normed['target_raw'] = data_file['target']\n",
    "data_normed['target'] = data_file['target_binary']\n",
    "\n",
    "# create data_set\n",
    "train , test = create_train_test_file(data_file, data_size, test_split)\n",
    "train_sample = train[model_features].values\n",
    "test_sample = test[model_features].values\n",
    " # Parse the values into the LSTM format\n",
    "train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "\n",
    "# reshape seems to add another list around every observation\n",
    "train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "\n",
    "# Build up the model\n",
    "BATCH_SIZE = 300\n",
    "no_features = train_data.shape[2]\n",
    "model = Sequential()\n",
    "model.add(LSTM(4,batch_input_shape = (None,look_back,no_features), return_sequences = True))\n",
    "model.add(LSTM(1, return_sequences = False, activation=\"softmax\"))\n",
    "model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "\n",
    "EPOCH = 1000\n",
    "# train the model\n",
    "# verbose = 1 gives the output of the training.\n",
    "start_time = datetime.datetime.now()\n",
    "lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose=1)\n",
    "run_time = datetime.datetime.now() - start_time\n",
    "# run training on the test data\n",
    "results = model.predict(test_data)\n",
    "# The % threshold needed to trigger a signal either way\n",
    "thold = 0.55\n",
    "predicted = [signal(i, thold) for i in results]\n",
    "acc_score = get_accuracy(predicted, test_target)\n",
    "predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "# calculate the returns of the signal\n",
    "test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "# no shift needed as we have already done that in previous step\n",
    "test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "strat_return = test_results['strat_returns'].sum()\n",
    "information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "\n",
    "# Store the data as needed\n",
    "performance_store['data_size'].append(data_size)\n",
    "performance_store['epochs'].append(EPOCH)\n",
    "performance_store['Accuracy_Score'].append(acc_score)\n",
    "performance_store['Info_Ratio'].append(information_ratio)\n",
    "performance_store['run_time'].append(run_time)\n",
    "performance_df = pd.DataFrame(performance_store)\n",
    "save_results = r\"/storage/test_result_size%s_lkbk%s_Epochs%s_thold%s.csv\" % (data_size, look_back, EPOCH, thold)\n",
    "test_results.to_csv( save_results,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is a simple implementation of the \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_trading_module import create_train_test_file, create_dataset, get_accuracy, signal\n",
    "import datetime\n",
    "\n",
    "file_location = r\"/storage/ccyData.csv\"\n",
    "\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"epochs\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "look_back = 60 # [21, 66]\n",
    "trade_horizon = 24\n",
    "test_buffer = 5\n",
    "data_size = 5000\n",
    "test_split = 0.5\n",
    "total_data_needed = int(data_size*(1 + test_split)) + test_buffer\n",
    "###### Set Targets ##############\n",
    "trade_horizon = 24 # in hours\n",
    "use_risk_adjusted = False # if True: training on the sharpe return else raw\n",
    "concat_results = True \n",
    "use_classifier = True\n",
    "################### Standardise Entire Dataset using rolling lookback windows #######\n",
    "# roughly ~3 yrs of data \n",
    "window = 17500\n",
    "use_pca = 3 # if = 0 then implies do not use pca in the model\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "start_row = 0\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "start_row = 85000\n",
    "use_separated_chunk = False # Use a rolling window to train and test\n",
    "data_normed, model_features, features_to_standardise = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca)\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "while start_row < data_normed.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += total_data_needed\n",
    "    if concat_results:\n",
    "        # in this instance, we can to add to the start row first before chunking the data\n",
    "        start_row += test_split\n",
    "        # we are training on all data available up until that point, and testing x timeperiods ahead\n",
    "        trunc_data = data_normed.loc[:start_row,:]  \n",
    "    else:\n",
    "        # this rolls the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        trunc_data = data_normed.loc[start_row:,:]\n",
    "        start_row += data_size\n",
    "    # standardise the data\n",
    "    #################### Set up training and testing ########################\n",
    "    \n",
    "    # create data_set\n",
    "    train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer)\n",
    "    if test.shape[0] <= (look_back+test_buffer+trade_horizon):\n",
    "        break\n",
    "    if use_pca > 0:\n",
    "        train, test, var_explained = get_pca_features(train,test, features_to_standardise, use_pca)\n",
    "    train_sample = train[model_features].values\n",
    "    test_sample = test[model_features].values\n",
    "\n",
    "     # Parse the values into the LSTM format\n",
    "    train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "    test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "    \n",
    "    # reshape seems to add another list around every observation\n",
    "    train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "    train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "    test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "    test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "    #### Set up model parameters\n",
    "    # Build up the model\n",
    "    BATCH_SIZE = 300\n",
    "    no_features = train_data.shape[2]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(4,batch_input_shape = (None,look_back,no_features), return_sequences = True))\n",
    "    model.add(LSTM(1, return_sequences = False, activation=\"softmax\"))\n",
    "    model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    \n",
    "    EPOCH = 350\n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    start_time = datetime.datetime.now()\n",
    "    lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose= 1)\n",
    "    run_time = datetime.datetime.now() - start_time\n",
    "    # run training on the test data\n",
    "    results = model.predict(test_data)\n",
    "    # The % threshold needed to trigger a signal either way\n",
    "    thold = 0.55\n",
    "    predicted = [signal(i, thold) for i in results]\n",
    "    acc_score = get_accuracy(predicted, test_target)\n",
    "    # This needs to change to handle the change in the target\n",
    "    predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "    test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "    # calculate the returns of the signal\n",
    "    test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "    # no shift needed as we have already done that in previous step\n",
    "    test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "    test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "    strat_return = test_results['strat_returns'].sum()\n",
    "    information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "    \n",
    "    # Store the data as needed\n",
    "    performance_store['data_size'].append(data_size)\n",
    "    performance_store['epochs'].append(EPOCH)\n",
    "    performance_store['Accuracy_Score'].append(acc_score)\n",
    "    performance_store['Info_Ratio'].append(information_ratio)\n",
    "    performance_store['run_time'].append(run_time)\n",
    "    performance_store['train_date_st'].append(trunc_data['Date'].iloc[0])\n",
    "    performance_store['test_date_st'].append(test_results['Date'].iloc[0])\n",
    "    performance_df = pd.DataFrame(performance_store)\n",
    "    save_results = r\"/storage/test_result_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)\n",
    "    test_results.to_csv(save_results,index = False)\n",
    "    performance_df.to_csv(r\"/storage/performance_df_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paperspace code for looping through the data series\n",
    "\n",
    "model = Sequential()\n",
    "###################### Import Data ##################################\n",
    "# data_file = pd.read_csv(r\"/storage/eurusd_train_normed.csv\")\n",
    "data_file = pd.read_csv(r\"/storage/ccyData.csv\")\n",
    "data_file = data_file.replace(np.nan, 0)\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"epochs\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "###################### Add Parameters ##################################################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "look_back = 66 # [21, 66]\n",
    "trade_horizon = 24\n",
    "model_features = [\"spot_v_HF\", \"spot_v_MF\", \"spot_v_LF\", \"HF_ema_diff\",\n",
    "                   \"MF_ema_diff\",\"LF_ema_diff\", \"LDN\", \"NY\", \"Asia\", \"target\"]\n",
    "test_buffer = 5\n",
    "data_size = 2500\n",
    "test_split = 0.75\n",
    "total_data_needed = int(data_size*(1 + test_split)) + test_buffer\n",
    "\n",
    "while start_row < data_file.shape[0]:\n",
    "    # first check if there is enough data left\n",
    "    if (start_row + total_data_needed) > data_file.shape[0]:\n",
    "        # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "        trunc_data = data_file.iloc[-total_data_needed:,:]\n",
    "    # we need to increment over the data size\n",
    "    if use_separated_chunk:\n",
    "        # this means we jump across the full previous train and test data\n",
    "        start_row += total_data_needed\n",
    "        trunc_data = data_file.loc[start_row:,:]\n",
    "    if use_rolling_chunk:\n",
    "        # this rools the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "        start_row += data_size\n",
    "        trunc_data = data_file.loc[start_row:,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\venv\\trading_eng\\ml_trading_module.py:78: RuntimeWarning: divide by zero encountered in true_divide\n  return data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).sum().values[::-1]/data_df['logret'].iloc[::-1].shift(2).rolling(trade_horizon).std().values[::-1]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-af74e1a3b9b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m      \u001b[0;31m# Parse the values into the LSTM format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mtrain_data\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnull_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\venv\\trading_eng\\model_functions.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(dataset, populate_target, look_back, test)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpopulate_target\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtarget_dates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlook_back\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "##### FOR PCA features  && train on random data!####\n",
    "# This file is a simple implementation of the \n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trading_eng.ml_trading_module import *\n",
    "from trading_eng.model_functions import *\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "file_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyData.csv\"\n",
    "performance_store = {\"data_size\" : [], \"Accuracy_Score\" : [], \"ntree\": [],\n",
    "                     \"Info_Ratio\" : [], \"run_time\" : [], \"train_date_st\": [], \"test_date_st\": []}\n",
    "params_dict = set_params_random_forests()\n",
    "lstm_dict = set_params_LSTM()\n",
    "use_random_train_data = params_dict['use_random_train_data']\n",
    "########################### Set Model Paramaters #############################\n",
    "# this looks back over a set period as the memory for the LSTM\n",
    "EPOCH = lstm_dict['EPOCH']\n",
    "first_layer = lstm_dict['first_layer']\n",
    "second_layer = lstm_dict['second_layer']\n",
    "look_back = lstm_dict['look_back'] \n",
    "\n",
    "# [i for i in range(25,301,25)] # [21, 66]\n",
    "# if running pca, max features can only be same or less than the full total of features\n",
    "test_buffer = params_dict['test_buffer']\n",
    "data_size = params_dict['data_size'] #  initially using 1500 training points\n",
    "# I.e. append the data into one df over each training window, but also use all available up until that point\n",
    "concat_results = params_dict['concat_results']\n",
    "# if the number is > 1, then the code takes that as the number of test points you want to use\n",
    "test_split = params_dict['test_split'] # roughly one month test ahead, which is a one month retrain period\n",
    "# signal threshold, when using classifier\n",
    "thold = params_dict['thold']\n",
    "total_data_needed = get_total_data_needed(test_split,data_size,test_buffer)\n",
    "# standardisation window\n",
    "window = params_dict['window']\n",
    "###### Set Targets ##############\n",
    "trade_horizon = params_dict['trade_horizon'] # in hours\n",
    "use_risk_adjusted = params_dict['use_risk_adjusted'] # if True: training on the sharpe return else raw\n",
    "use_binary = params_dict['use_binary'] # set to true if you are using the risk adjusted and want it binary for classification score\n",
    "use_classifier = params_dict['use_classifier']\n",
    "use_pca = params_dict['use_pca'] # if = 0 then implies do not use pca in the model\n",
    "use_separated_chunk = params_dict['use_separated_chunk']\n",
    "################### Standardise Entire Dataset using rolling lookback windows ###############\n",
    "data_normed, model_features, features_to_standardise = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca,use_random_train_data)\n",
    "#data_normed = data_normed.replace(np.nan, 0)\n",
    "start_row = data_size\n",
    " # Use a rolling window to train and test\n",
    "################ Loop through the full dataset in terms of the training and testing.\n",
    "if use_random_train_data:\n",
    "    random_data_location = r\"C:\\Users\\edgil\\Documents\\Masters\\dissertation\\code64\\data_set\\CcyRandomTrend.csv\"\n",
    "    train, model_features, features_to_standardise = initialise_process(random_data_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, use_random_train_data)\n",
    "    test, model_features, features_to_standardise  = initialise_process(file_location, trade_horizon, \n",
    "                                                 window, use_risk_adjusted, use_pca, \n",
    "                                                use_random_train_data= False)\n",
    "    if test.shape[0] <= (look_back+test_buffer+trade_horizon):\n",
    "        sys.exit()\n",
    "    if use_pca > 0:\n",
    "        train, test, var_explained = get_pca_features(train,test, features_to_standardise, use_pca)\n",
    "    train_sample = train[model_features].values\n",
    "    test_sample = test[model_features].values\n",
    "\n",
    "     # Parse the values into the LSTM format\n",
    "    train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "    test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "    \n",
    "    # reshape seems to add another list around every observation\n",
    "    train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "    train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "    test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "    test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "    #### Set up model parameters\n",
    "    # Build up the model\n",
    "    BATCH_SIZE = 300\n",
    "    no_features = train_data.shape[2]\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(first_layer,batch_input_shape = (None,look_back,no_features), return_sequences = True))\n",
    "    model.add(LSTM(second_layer, return_sequences = False, activation=\"softmax\"))\n",
    "    model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    # verbose = 1 gives the output of the training.\n",
    "    start_time = datetime.datetime.now()\n",
    "    lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose= 1)\n",
    "    run_time = datetime.datetime.now() - start_time\n",
    "    # run training on the test data\n",
    "    results = model.predict(test_data)\n",
    "    # The % threshold needed to trigger a signal either way\n",
    "    predicted = [signal(i, thold) for i in results]\n",
    "    acc_score = get_accuracy(predicted, test_target)\n",
    "    # This needs to change to handle the change in the target\n",
    "    predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "    test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "    # calculate the returns of the signal\n",
    "    test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "    # no shift needed as we have already done that in previous step\n",
    "    test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "    test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "    strat_return = test_results['strat_returns'].sum()\n",
    "    information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "    \n",
    "    # Store the data as needed\n",
    "    performance_store['data_size'].append(data_size)\n",
    "    performance_store['epochs'].append(EPOCH)\n",
    "    performance_store['Accuracy_Score'].append(acc_score)\n",
    "    performance_store['Info_Ratio'].append(information_ratio)\n",
    "    performance_store['run_time'].append(run_time)\n",
    "    performance_store['train_date_st'].append(trunc_data['Date'].iloc[0])\n",
    "    performance_store['test_date_st'].append(test_results['Date'].iloc[0])\n",
    "    performance_df = pd.DataFrame(performance_store)\n",
    "    save_results = r\"/storage/test_result_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)\n",
    "    test_results.to_csv(save_results,index = False)\n",
    "    performance_df.to_csv(r\"/storage/performance_df_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold))\n",
    "else:\n",
    "    # if not using random data then move to the normal method.\n",
    "    ################ Loop through the full dataset in terms of the training and testing.\n",
    "    while start_row < data_normed.shape[0]:\n",
    "        # first check if there is enough data left\n",
    "        if (start_row + total_data_needed) > data_normed.shape[0]:\n",
    "            # if we are about to go over the limit, then just return the last data_size + test size proportion of data\n",
    "            trunc_data = data_normed.iloc[-total_data_needed:,:]\n",
    "        # we need to increment over the data size\n",
    "        if use_separated_chunk:\n",
    "            # this means we jump across the full previous train and test data\n",
    "            trunc_data = data_normed.loc[start_row:,:]\n",
    "            start_row += total_data_needed\n",
    "        if concat_results:\n",
    "            # in this instance, we can to add to the start row first before chunking the data\n",
    "            start_row += test_split\n",
    "            # we are training on all data available up until that point, and testing x timeperiods ahead\n",
    "            trunc_data = data_normed.loc[:start_row,:]  \n",
    "        else:\n",
    "            # this rolls the data so that the new training will overlap on the old test set and create a new separated test set\n",
    "            trunc_data = data_normed.loc[start_row:,:]\n",
    "            start_row += data_size\n",
    "        # standardise the data\n",
    "        #################### Set up training and testing ########################\n",
    "        \n",
    "        # create data_set\n",
    "        train , test = create_train_test_file(trunc_data, data_size, test_split, test_buffer)\n",
    "        if test.shape[0] <= (look_back+test_buffer+trade_horizon):\n",
    "            break\n",
    "        if use_pca > 0:\n",
    "            train, test, var_explained = get_pca_features(train,test, features_to_standardise, use_pca)\n",
    "        train_sample = train[model_features].values\n",
    "        test_sample = test[model_features].values\n",
    "    \n",
    "         # Parse the values into the LSTM format\n",
    "        train_data , train_target, null_dates = create_dataset(train_sample,False, look_back, test)\n",
    "        test_data, test_target, target_dates = create_dataset(test_sample, True, look_back, test)\n",
    "        \n",
    "        # reshape seems to add another list around every observation\n",
    "        train_data = train_data.reshape(train_data.shape[0], look_back, train_data.shape[2])\n",
    "        train_target = train_target.reshape(train_target.shape[0], 1)\n",
    "        test_data = test_data.reshape(test_data.shape[0], look_back, test_data.shape[2])\n",
    "        test_target = test_target.reshape(test_target.shape[0], 1)\n",
    "        #### Set up model parameters\n",
    "        # Build up the model\n",
    "        BATCH_SIZE = 300\n",
    "        no_features = train_data.shape[2]\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(4,batch_input_shape = (None,look_back,no_features), return_sequences = True))\n",
    "        model.add(LSTM(1, return_sequences = False, activation=\"softmax\"))\n",
    "        model.compile(loss = \"mean_absolute_error\", optimizer=\"adam\", metrics = ['accuracy'])\n",
    "        \n",
    "        EPOCH = 350\n",
    "        # train the model\n",
    "        # verbose = 1 gives the output of the training.\n",
    "        start_time = datetime.datetime.now()\n",
    "        lstm_engine = model.fit(train_data,train_target,epochs = EPOCH,validation_data=(test_data,test_target), verbose= 1)\n",
    "        run_time = datetime.datetime.now() - start_time\n",
    "        # run training on the test data\n",
    "        results = model.predict(test_data)\n",
    "        # The % threshold needed to trigger a signal either way\n",
    "        thold = 0.55\n",
    "        predicted = [signal(i, thold) for i in results]\n",
    "        acc_score = get_accuracy(predicted, test_target)\n",
    "        # This needs to change to handle the change in the target\n",
    "        predictions = pd.DataFrame({\"Date\" : target_dates,\"Predictions\": predicted})\n",
    "        test_results = pd.merge(test,predictions,how=\"left\", on=\"Date\").fillna(0)\n",
    "        # calculate the returns of the signal\n",
    "        test_results[\"scaled_signal\"] = test_results['Predictions'].shift(2).rolling(trade_horizon).sum()/trade_horizon\n",
    "        # no shift needed as we have already done that in previous step\n",
    "        test_results['strat_returns'] = test_results['logret']*test_results['scaled_signal']\n",
    "        test_results['strat_returns_sum'] = test_results['strat_returns'].cumsum()\n",
    "        strat_return = test_results['strat_returns'].sum()\n",
    "        information_ratio = (test_results['strat_returns'].mean()*260)/(test_results['strat_returns'].std()*np.sqrt(260))\n",
    "        \n",
    "        # Store the data as needed\n",
    "        performance_store['data_size'].append(data_size)\n",
    "        performance_store['epochs'].append(EPOCH)\n",
    "        performance_store['Accuracy_Score'].append(acc_score)\n",
    "        performance_store['Info_Ratio'].append(information_ratio)\n",
    "        performance_store['run_time'].append(run_time)\n",
    "        performance_store['train_date_st'].append(trunc_data['Date'].iloc[0])\n",
    "        performance_store['test_date_st'].append(test_results['Date'].iloc[0])\n",
    "        performance_df = pd.DataFrame(performance_store)\n",
    "        save_results = r\"/storage/test_result_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)\n",
    "        test_results.to_csv(save_results,index = False)\n",
    "        performance_df.to_csv(r\"/storage/performance_df_start_row%s_lkbk%s_Epochs%s_thold%s.csv\" % (start_row, look_back, EPOCH, thold)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
